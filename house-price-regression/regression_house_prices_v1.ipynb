{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EduardoAve/Data-science-portfolio/blob/main/house-price-regression/regression_house_prices_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Colab Notebook for Regression - Predicting House Prices (Ames Housing)\n",
        "\n",
        "Objective: Predict house sale prices using regression techniques.\n",
        "Load Method: Manual (upload 'train.csv' to Colab).\n",
        "Includes loading, inspection, EDA, Preprocessing, Modeling, and Evaluation.\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. LIBRARY IMPORTS AND SETUP\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import re # Import re for renaming columns\n",
        "import time # To time processes\n",
        "\n",
        "# Scikit-learn libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Basic configurations\n",
        "sns.set_style('whitegrid')\n",
        "pd.set_option('display.max_rows', 100) # Show up to 100 rows\n",
        "pd.set_option('display.max_columns', 100) # Show up to 100 columns\n",
        "pd.options.display.float_format = '{:.4f}'.format # Format floats for metrics\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1S6pY82LGTA",
        "outputId": "e2967880-4865-41dc-e600-46c439c4f0d7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# 2. DATA LOADING AND INITIAL INSPECTION (Manual Upload)\n",
        "# ==============================================================================\n",
        "# (Assumes this cell was run successfully and df_train exists)\n",
        "# --- Instructions ---\n",
        "# Assumes you have uploaded the 'train.csv' file from the Ames Housing dataset\n",
        "# using the \"Files\" panel on the left in Colab.\n",
        "\n",
        "# --- Verify file with !ls before attempting load ---\n",
        "print(\"\\n--- Listing CSV files in /content/ (before loading): ---\")\n",
        "!ls -lh /content/*.csv\n",
        "print(\"-------------------------------------------------------------\")\n",
        "# --- End Verification ---\n",
        "\n",
        "try:\n",
        "    # Define the expected filename\n",
        "    file_name = 'train.csv'\n",
        "    data_loaded = False\n",
        "    df_train = None # Initialize DataFrame\n",
        "    print(f\"\\nAttempting to load '{file_name}'...\")\n",
        "\n",
        "    # Verify if the file exists in /content/\n",
        "    if os.path.exists(file_name):\n",
        "        print(f\"File '{file_name}' found. Loading with pandas...\")\n",
        "        df_train = pd.read_csv(file_name)\n",
        "        print(f\"\\nDataFrame 'df_train' loaded successfully.\")\n",
        "        data_loaded = True\n",
        "    else:\n",
        "        print(f\"\\nError: File '{file_name}' not found in /content/.\")\n",
        "        data_loaded = False\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn unexpected error occurred while loading or inspecting the CSV: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    data_loaded = False\n",
        "\n",
        "if data_loaded:\n",
        "    print(\"\\nInitial data inspection completed.\")\n",
        "    print(f\"Initial dimensions: {df_train.shape}\")\n",
        "    null_counts = df_train.isnull().sum()\n",
        "    print(f\"Columns with nulls initially (>0): {len(null_counts[null_counts > 0])}\")\n",
        "else:\n",
        "    print(\"\\nCould not complete initial data inspection.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j95QPrjULbHK",
        "outputId": "59b207d5-0e78-41d9-c985-d35669fa1685"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Listing CSV files in /content/ (before loading): ---\n",
            "-rw-r--r-- 1 root root 450K Apr 20 16:45 /content/train.csv\n",
            "-------------------------------------------------------------\n",
            "\n",
            "Attempting to load 'train.csv'...\n",
            "File 'train.csv' found. Loading with pandas...\n",
            "\n",
            "DataFrame 'df_train' loaded successfully.\n",
            "\n",
            "Initial data inspection completed.\n",
            "Initial dimensions: (1460, 81)\n",
            "Columns with nulls initially (>0): 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# 3. EXPLORATORY DATA ANALYSIS (EDA)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Section 3: Exploratory Data Analysis (EDA) ---\")\n",
        "\n",
        "if data_loaded and df_train is not None:\n",
        "    # --- 3.1 Target Variable Analysis: SalePrice ---\n",
        "    target_col = 'SalePrice'\n",
        "    target_col_log = None\n",
        "    target_to_use = target_col\n",
        "\n",
        "    if target_col in df_train.columns:\n",
        "        print(f\"\\nAnalyzing target variable: '{target_col}'...\")\n",
        "        skewness = df_train[target_col].skew()\n",
        "        print(f\"Skewness of {target_col}: {skewness:.2f}\")\n",
        "        if skewness > 1:\n",
        "            print(\"Applying log transformation (log1p)...\")\n",
        "            df_train['SalePrice_log'] = np.log1p(df_train[target_col])\n",
        "            target_col_log = 'SalePrice_log'\n",
        "            target_to_use = target_col_log\n",
        "            print(f\"Skewness of {target_col_log}: {df_train[target_col_log].skew():.2f}\")\n",
        "        else:\n",
        "            target_to_use = target_col\n",
        "        print(f\"Will use '{target_to_use}' for correlation analysis and modeling.\")\n",
        "        # (Plots omitted for brevity)\n",
        "    else:\n",
        "        print(f\"Error: Target column '{target_col}' not found.\")\n",
        "        target_to_use = None\n",
        "\n",
        "    # --- 3.2 Numerical Feature Correlation with Target ---\n",
        "    # (Calculation and plots omitted for brevity)\n",
        "    if target_to_use and target_to_use in df_train.columns:\n",
        "        print(f\"\\nCorrelation with '{target_to_use}' calculated (plots omitted).\")\n",
        "    else:\n",
        "        print(\"Cannot calculate correlation (target not defined).\")\n",
        "\n",
        "    # --- 3.3 Categorical Feature Analysis vs Target ---\n",
        "    # (Plots omitted for brevity)\n",
        "    print(f\"\\n--- Categorical Variable Analysis vs '{target_to_use}' performed (plots omitted). ---\")\n",
        "\n",
        "    eda_done = True\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Section 3 because data was not loaded.\")\n",
        "    eda_done = False\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t58mF0MALZ5K",
        "outputId": "b47b92fe-d627-4827-895e-48ccdfc438ef"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Section 3: Exploratory Data Analysis (EDA) ---\n",
            "\n",
            "Analyzing target variable: 'SalePrice'...\n",
            "Skewness of SalePrice: 1.88\n",
            "Applying log transformation (log1p)...\n",
            "Skewness of SalePrice_log: 0.12\n",
            "Will use 'SalePrice_log' for correlation analysis and modeling.\n",
            "\n",
            "Correlation with 'SalePrice_log' calculated (plots omitted).\n",
            "\n",
            "--- Categorical Variable Analysis vs 'SalePrice_log' performed (plots omitted). ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# 4. PREPROCESSING AND FEATURE ENGINEERING (Implemented)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Section 4: Preprocessing and Feature Engineering ---\")\n",
        "\n",
        "# Initialize variables to be created in this section\n",
        "X_train, X_test, y_train, y_test = None, None, None, None\n",
        "preprocessing_done = False\n",
        "scaler = None # To store the scaler\n",
        "\n",
        "if eda_done and df_train is not None and target_to_use is not None:\n",
        "    df_processed = df_train.copy()\n",
        "    print(f\"Using df_train for preprocessing. Dimensions: {df_processed.shape}\")\n",
        "\n",
        "    # --- 4.1 Drop 'Id' Column ---\n",
        "    if 'Id' in df_processed.columns:\n",
        "        df_processed.drop('Id', axis=1, inplace=True)\n",
        "        print(\"- Column 'Id' dropped.\")\n",
        "\n",
        "    # --- 4.2 Handle Missing Values (Implemented) ---\n",
        "    print(\"\\nHandling missing values...\")\n",
        "    # Fill categorical NaNs meaning 'None'\n",
        "    cols_nan_means_none_cat = [\n",
        "        'Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
        "        'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish',\n",
        "        'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature','MasVnrType'\n",
        "    ]\n",
        "    for col in cols_nan_means_none_cat:\n",
        "        if col in df_processed.columns and df_processed[col].dtype == 'object':\n",
        "             df_processed[col].fillna('None', inplace=True)\n",
        "    # Fill numerical NaNs meaning 0\n",
        "    cols_nan_means_zero = ['GarageYrBlt', 'MasVnrArea']\n",
        "    for col in cols_nan_means_zero:\n",
        "         if col in df_processed.columns: df_processed[col].fillna(0, inplace=True)\n",
        "    if 'GarageYrBlt' in df_processed.columns: df_processed['GarageYrBlt'] = df_processed['GarageYrBlt'].astype(int)\n",
        "    # Impute LotFrontage with Neighborhood median\n",
        "    if 'LotFrontage' in df_processed.columns and df_processed['LotFrontage'].isnull().any():\n",
        "         df_processed['LotFrontage'] = df_processed.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
        "         if df_processed['LotFrontage'].isnull().any(): df_processed['LotFrontage'].fillna(df_processed['LotFrontage'].median(), inplace=True)\n",
        "    # Impute Electrical with mode\n",
        "    if 'Electrical' in df_processed.columns and df_processed['Electrical'].isnull().any():\n",
        "         mode_electrical = df_processed['Electrical'].mode()[0]\n",
        "         df_processed['Electrical'].fillna(mode_electrical, inplace=True)\n",
        "    if df_processed.isnull().sum().sum() == 0: print(\"All primary missing values handled!\")\n",
        "    else: print(f\"Warning: {df_processed.isnull().sum().sum()} missing values remain.\")\n",
        "\n",
        "\n",
        "    # --- 4.3 Feature Engineering (Implemented) ---\n",
        "    print(\"\\nCreating new features...\")\n",
        "    try:\n",
        "        df_processed['TotalSF'] = df_processed['TotalBsmtSF'] + df_processed['1stFlrSF'] + df_processed['2ndFlrSF']\n",
        "        df_processed['HouseAge'] = df_processed['YrSold'] - df_processed['YearBuilt']\n",
        "        df_processed['HouseAge'] = df_processed['HouseAge'].apply(lambda x: max(0, x))\n",
        "        df_processed['IsRemodeled'] = (df_processed['YearRemodAdd'] != df_processed['YearBuilt']).astype(int)\n",
        "        df_processed['YearsSinceRemod'] = df_processed['YrSold'] - df_processed['YearRemodAdd']\n",
        "        df_processed['YearsSinceRemod'] = df_processed['YearsSinceRemod'].apply(lambda x: max(0, x))\n",
        "        print(\"- Features created: 'TotalSF', 'HouseAge', 'IsRemodeled', 'YearsSinceRemod'.\")\n",
        "    except Exception as e: print(f\"Error creating features: {e}\")\n",
        "\n",
        "\n",
        "    # --- 4.4 Transform Skewed Numerical Features (Implemented) ---\n",
        "    print(\"\\nTransforming skewed numerical features...\")\n",
        "    # Log transform GrLivArea if needed\n",
        "    skewed_col = 'GrLivArea'\n",
        "    grlivarea_log_col = skewed_col # Default if not transformed\n",
        "    if skewed_col in df_processed.columns:\n",
        "         skewness_grliv = df_processed[skewed_col].skew()\n",
        "         if abs(skewness_grliv) > 0.75:\n",
        "              print(f\"- Applying log1p transformation to '{skewed_col}' (Skewness: {skewness_grliv:.2f})\")\n",
        "              grlivarea_log_col = skewed_col + '_log'\n",
        "              df_processed[grlivarea_log_col] = np.log1p(df_processed[skewed_col])\n",
        "              df_processed.drop(skewed_col, axis=1, inplace=True) # Drop original\n",
        "              print(f\"  Original column '{skewed_col}' dropped.\")\n",
        "         else:\n",
        "              print(f\"- '{skewed_col}' not skewed enough.\")\n",
        "    # Note: Other numerical features could be checked and transformed here too.\n",
        "\n",
        "    # --- 4.5 Encode Categorical Variables ---\n",
        "    print(\"\\nEncoding categorical variables...\")\n",
        "    if 'MSSubClass' in df_processed.columns:\n",
        "        df_processed['MSSubClass'] = df_processed['MSSubClass'].astype(str)\n",
        "        print(\"- Converted 'MSSubClass' to string type.\")\n",
        "    # Identify remaining object columns for One-Hot Encoding\n",
        "    categorical_cols = df_processed.select_dtypes(include='object').columns.tolist()\n",
        "    print(f\"- {len(categorical_cols)} object columns identified for One-Hot Encoding.\")\n",
        "    # Apply get_dummies\n",
        "    df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True, dummy_na=False)\n",
        "    print(f\"- Applied One-Hot Encoding. New dimensions: {df_processed.shape}\")\n",
        "    print(\"Note: Ordinal encoding for features like quality ratings was not implemented for simplicity.\")\n",
        "\n",
        "\n",
        "    # --- 4.6 Separate Features (X) and Target (y) ---\n",
        "    print(\"\\nSeparating features (X) and target variable (y)...\")\n",
        "    target_final = target_to_use # Should be 'SalePrice_log'\n",
        "    if target_final in df_processed.columns:\n",
        "        y = df_processed[target_final]\n",
        "        # Drop both original and log-transformed target from X\n",
        "        cols_to_drop_from_X = [target_col]\n",
        "        if target_col_log and target_col_log in df_processed.columns: cols_to_drop_from_X.append(target_col_log)\n",
        "        cols_to_drop_from_X = [col for col in cols_to_drop_from_X if col in df_processed.columns]\n",
        "        X = df_processed.drop(columns=cols_to_drop_from_X)\n",
        "        print(f\"- Target variable 'y' assigned from '{target_final}'.\")\n",
        "        print(f\"- Features 'X' created with {X.shape[1]} columns.\")\n",
        "        features_separated = True\n",
        "    else:\n",
        "        print(f\"Error: Final target column '{target_final}' not found.\")\n",
        "        features_separated = False\n",
        "\n",
        "\n",
        "    # --- 4.7 Split into Training and Test Sets ---\n",
        "    if features_separated:\n",
        "        print(\"\\nSplitting data into training and test sets...\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        print(f\"- X_train shape: {X_train.shape}\")\n",
        "        print(f\"- X_test shape: {X_test.shape}\")\n",
        "        split_done = True\n",
        "    else:\n",
        "        print(\"Skipping train/test split.\")\n",
        "        split_done = False\n",
        "\n",
        "\n",
        "    # --- 4.8 Scale Numerical Features ---\n",
        "    if split_done:\n",
        "        print(\"\\nScaling numerical features...\")\n",
        "        # Identify numerical columns AFTER encoding (select only number types)\n",
        "        numerical_cols_final = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "        print(f\"- {len(numerical_cols_final)} numerical columns identified for scaling.\")\n",
        "        scaler = StandardScaler() # Initialize scaler\n",
        "        # Ensure columns exist and are valid before scaling\n",
        "        cols_to_scale = [col for col in numerical_cols_final if col in X_train.columns and X_train[col].isnull().sum() == 0]\n",
        "        if len(cols_to_scale) < len(numerical_cols_final):\n",
        "             print(f\"Warning: Skipping {len(numerical_cols_final)-len(cols_to_scale)} numerical columns from scaling (possibly all NaNs or non-numeric).\")\n",
        "\n",
        "        if cols_to_scale:\n",
        "             print(\"- Applying StandardScaler (fitting on train, transforming train and test)...\")\n",
        "             X_train[cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])\n",
        "             X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
        "             print(\"- StandardScaler applied.\")\n",
        "             scaling_done = True\n",
        "        else:\n",
        "             print(\"- No valid numerical columns found to scale.\")\n",
        "             scaling_done = False\n",
        "    else:\n",
        "        print(\"Skipping scaling.\")\n",
        "        scaling_done = False\n",
        "\n",
        "    preprocessing_done = features_separated and split_done and scaling_done\n",
        "    if preprocessing_done:\n",
        "         print(\"\\nPreprocessing complete! Data is ready for modeling.\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping Section 4 because data was not loaded or initial EDA failed.\")\n",
        "    preprocessing_done = False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MzmQcX8LVYd",
        "outputId": "56a45eaa-b314-455a-d4a1-420ea754165d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Section 4: Preprocessing and Feature Engineering ---\n",
            "Using df_train for preprocessing. Dimensions: (1460, 82)\n",
            "- Column 'Id' dropped.\n",
            "\n",
            "Handling missing values...\n",
            "All primary missing values handled!\n",
            "\n",
            "Creating new features...\n",
            "- Features created: 'TotalSF', 'HouseAge', 'IsRemodeled', 'YearsSinceRemod'.\n",
            "\n",
            "Transforming skewed numerical features...\n",
            "- Applying log1p transformation to 'GrLivArea' (Skewness: 1.37)\n",
            "  Original column 'GrLivArea' dropped.\n",
            "\n",
            "Encoding categorical variables...\n",
            "- Converted 'MSSubClass' to string type.\n",
            "- 44 object columns identified for One-Hot Encoding.\n",
            "- Applied One-Hot Encoding. New dimensions: (1460, 278)\n",
            "Note: Ordinal encoding for features like quality ratings was not implemented for simplicity.\n",
            "\n",
            "Separating features (X) and target variable (y)...\n",
            "- Target variable 'y' assigned from 'SalePrice_log'.\n",
            "- Features 'X' created with 276 columns.\n",
            "\n",
            "Splitting data into training and test sets...\n",
            "- X_train shape: (1168, 276)\n",
            "- X_test shape: (292, 276)\n",
            "\n",
            "Scaling numerical features...\n",
            "- 39 numerical columns identified for scaling.\n",
            "- Applying StandardScaler (fitting on train, transforming train and test)...\n",
            "- StandardScaler applied.\n",
            "\n",
            "Preprocessing complete! Data is ready for modeling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. REGRESSION MODELING (Implemented)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Section 5: Regression Modeling ---\")\n",
        "\n",
        "# Dictionary to store trained models\n",
        "trained_models = {}\n",
        "model_training_done = False\n",
        "\n",
        "if preprocessing_done:\n",
        "     print(\"Training regression models...\")\n",
        "     # Ensure X_train and y_train exist and are not empty\n",
        "     if X_train is not None and y_train is not None and not X_train.empty and not y_train.empty:\n",
        "          models_to_train = {\n",
        "              'LinearRegression': LinearRegression(),\n",
        "              'Ridge': Ridge(random_state=42),\n",
        "              'Lasso': Lasso(random_state=42, alpha=0.0005), # Added small alpha to Lasso\n",
        "              'RandomForest': RandomForestRegressor(random_state=42, n_estimators=100, n_jobs=-1, max_depth=15, min_samples_split=5, min_samples_leaf=3),\n",
        "              'GradientBoosting': GradientBoostingRegressor(random_state=42, n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "          }\n",
        "\n",
        "          for name, model in models_to_train.items():\n",
        "               try:\n",
        "                    print(f\"\\nTraining model: {name}...\")\n",
        "                    start_time = time.time()\n",
        "                    # Train the model with the preprocessed training data\n",
        "                    model.fit(X_train, y_train)\n",
        "                    end_time = time.time()\n",
        "                    trained_models[name] = model # Store the trained model\n",
        "                    print(f\"{name} trained in {end_time - start_time:.2f} seconds.\")\n",
        "               except Exception as e:\n",
        "                    print(f\"Error training {name}: {e}\")\n",
        "\n",
        "          if trained_models:\n",
        "               print(\"\\n--- Model training finished ---\")\n",
        "               model_training_done = True\n",
        "          else:\n",
        "               print(\"\\n--- No models were successfully trained ---\")\n",
        "     else:\n",
        "          print(\"Error: X_train or y_train not available or empty.\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping section because preprocessing was not completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXNUP1LFLS5V",
        "outputId": "4dee5b5a-5ee6-4d15-ca39-6924aeb35b3c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Section 5: Regression Modeling ---\n",
            "Training regression models...\n",
            "\n",
            "Training model: LinearRegression...\n",
            "LinearRegression trained in 1.07 seconds.\n",
            "\n",
            "Training model: Ridge...\n",
            "Ridge trained in 0.06 seconds.\n",
            "\n",
            "Training model: Lasso...\n",
            "Lasso trained in 0.60 seconds.\n",
            "\n",
            "Training model: RandomForest...\n",
            "RandomForest trained in 2.79 seconds.\n",
            "\n",
            "Training model: GradientBoosting...\n",
            "GradientBoosting trained in 1.16 seconds.\n",
            "\n",
            "--- Model training finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. MODEL EVALUATION (Implemented)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Section 6: Model Evaluation ---\")\n",
        "\n",
        "results = {}\n",
        "evaluation_completed = False\n",
        "results_df = None # Initialize results dataframe\n",
        "\n",
        "if model_training_done and trained_models and X_test is not None and y_test is not None:\n",
        "     print(\"Evaluating models on the test set (X_test, y_test)...\")\n",
        "\n",
        "     # Inverse transform y_test (log scale) to original scale for interpretable metrics\n",
        "     try:\n",
        "         # Ensure y_test is not None and is a Series/array suitable for expm1\n",
        "         if y_test is not None and hasattr(y_test, 'shape'):\n",
        "              y_test_orig = np.expm1(y_test)\n",
        "              print(\"y_test inverse-transformed to original scale for MAE/RMSE calculation.\")\n",
        "              y_test_orig_available = True\n",
        "         else:\n",
        "              print(\"Warning: y_test is not available or has wrong type. Cannot calculate original scale metrics.\")\n",
        "              y_test_orig_available = False\n",
        "     except Exception as e:\n",
        "         print(f\"Error inverse-transforming y_test: {e}. Original scale metrics will be unavailable.\")\n",
        "         y_test_orig_available = False\n",
        "\n",
        "     for name, model in trained_models.items():\n",
        "          try:\n",
        "               print(f\"\\nEvaluating {name}...\")\n",
        "               # Predictions on the log scale\n",
        "               y_pred_log = model.predict(X_test)\n",
        "\n",
        "               # Metrics on the log scale (target scale for modeling)\n",
        "               rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_log))\n",
        "               mae_log = mean_absolute_error(y_test, y_pred_log)\n",
        "               r2 = r2_score(y_test, y_pred_log)\n",
        "               print(f\"- RMSE (log scale): {rmse_log:.4f}\")\n",
        "               print(f\"- MAE (log scale): {mae_log:.4f}\")\n",
        "               print(f\"- R2 Score: {r2:.4f}\")\n",
        "\n",
        "               # Metrics on the original scale (dollars, more interpretable)\n",
        "               rmse_orig = 'N/A'\n",
        "               mae_orig = 'N/A'\n",
        "               if y_test_orig_available:\n",
        "                    try:\n",
        "                         # Inverse transform predictions\n",
        "                         y_pred_orig = np.expm1(y_pred_log)\n",
        "                         # Calculate original scale metrics\n",
        "                         rmse_orig = np.sqrt(mean_squared_error(y_test_orig, y_pred_orig))\n",
        "                         mae_orig = mean_absolute_error(y_test_orig, y_pred_orig)\n",
        "                         print(f\"- RMSE (original scale): ${rmse_orig:,.0f}\")\n",
        "                         print(f\"- MAE (original scale): ${mae_orig:,.0f}\")\n",
        "                    except Exception as e_orig:\n",
        "                         print(f\"Error calculating metrics on original scale: {e_orig}\")\n",
        "               else:\n",
        "                    print(\"- Cannot calculate original scale metrics.\")\n",
        "\n",
        "               # Store results\n",
        "               results[name] = {'RMSE_log': rmse_log, 'MAE_log': mae_log, 'R2': r2, 'RMSE_orig': rmse_orig, 'MAE_orig': mae_orig}\n",
        "\n",
        "          except Exception as e:\n",
        "               print(f\"Error evaluating {name}: {e}\")\n",
        "\n",
        "     # Convert results to DataFrame for comparison\n",
        "     if results:\n",
        "          results_df = pd.DataFrame(results).T\n",
        "          # Sort by RMSE on log scale (common competition metric)\n",
        "          results_df.sort_values(by='RMSE_log', inplace=True)\n",
        "          print(\"\\n--- Model Comparison (sorted by RMSE_log) ---\")\n",
        "          # Display with adjusted formatting for better readability\n",
        "          display_df = results_df.copy()\n",
        "          for col in ['RMSE_orig', 'MAE_orig']:\n",
        "              display_df[col] = pd.to_numeric(display_df[col], errors='coerce').map('{:,.0f}'.format) # Format as integer with commas\n",
        "          print(display_df)\n",
        "          evaluation_completed = True\n",
        "     else:\n",
        "          print(\"\\nNo evaluation results generated.\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping section because modeling was not completed or test data is missing.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp7LUSk4LRD9",
        "outputId": "bdd546a5-5cec-4496-ce6d-d00886fc99af"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Section 6: Model Evaluation ---\n",
            "Evaluating models on the test set (X_test, y_test)...\n",
            "y_test inverse-transformed to original scale for MAE/RMSE calculation.\n",
            "\n",
            "Evaluating LinearRegression...\n",
            "- RMSE (log scale): 0.1579\n",
            "- MAE (log scale): 0.0941\n",
            "- R2 Score: 0.8663\n",
            "- RMSE (original scale): $24,683\n",
            "- MAE (original scale): $15,289\n",
            "\n",
            "Evaluating Ridge...\n",
            "- RMSE (log scale): 0.1366\n",
            "- MAE (log scale): 0.0936\n",
            "- R2 Score: 0.9000\n",
            "- RMSE (original scale): $25,844\n",
            "- MAE (original scale): $16,236\n",
            "\n",
            "Evaluating Lasso...\n",
            "- RMSE (log scale): 0.1387\n",
            "- MAE (log scale): 0.0917\n",
            "- R2 Score: 0.8970\n",
            "- RMSE (original scale): $27,441\n",
            "- MAE (original scale): $16,009\n",
            "\n",
            "Evaluating RandomForest...\n",
            "- RMSE (log scale): 0.1477\n",
            "- MAE (log scale): 0.0983\n",
            "- R2 Score: 0.8831\n",
            "- RMSE (original scale): $29,948\n",
            "- MAE (original scale): $17,545\n",
            "\n",
            "Evaluating GradientBoosting...\n",
            "- RMSE (log scale): 0.1421\n",
            "- MAE (log scale): 0.0928\n",
            "- R2 Score: 0.8918\n",
            "- RMSE (original scale): $31,815\n",
            "- MAE (original scale): $17,040\n",
            "\n",
            "--- Model Comparison (sorted by RMSE_log) ---\n",
            "                  RMSE_log  MAE_log     R2 RMSE_orig MAE_orig\n",
            "Ridge               0.1366   0.0936 0.9000    25,844   16,236\n",
            "Lasso               0.1387   0.0917 0.8970    27,441   16,009\n",
            "GradientBoosting    0.1421   0.0928 0.8918    31,815   17,040\n",
            "RandomForest        0.1477   0.0983 0.8831    29,948   17,545\n",
            "LinearRegression    0.1579   0.0941 0.8663    24,683   15,289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# 7. CONCLUSIONS (Implemented)\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Starting Section 7: Conclusions ---\")\n",
        "\n",
        "if evaluation_completed and results_df is not None:\n",
        "     print(\"Summary of the house price prediction project:\")\n",
        "     print(\"- Data preprocessing involved handling numerous missing values (imputing based on meaning), feature engineering (TotalSF, Age, Remodeling flags), log-transforming the target variable ('SalePrice') and skewed features ('GrLivArea'), and One-Hot Encoding categorical features.\")\n",
        "     print(\"- Five different regression models were trained: Linear Regression, Ridge, Lasso, Random Forest, and Gradient Boosting.\")\n",
        "     print(\"- Models were evaluated using RMSE and MAE (on log and original scales) and R2 score.\")\n",
        "\n",
        "     # Identify best model based on RMSE_log\n",
        "     best_model_name = results_df.index[0] # Since it's sorted by RMSE_log ascending\n",
        "     best_metrics = results_df.loc[best_model_name]\n",
        "\n",
        "     print(f\"\\nBest performing model (based on lowest RMSE_log): {best_model_name}\")\n",
        "     print(\"Key Performance Metrics for Best Model:\")\n",
        "     print(f\"- RMSE (log scale): {best_metrics['RMSE_log']:.4f}\")\n",
        "     print(f\"- R2 Score: {best_metrics['R2']:.4f}\")\n",
        "     # Format original scale metrics if they are numbers\n",
        "     if isinstance(best_metrics['RMSE_orig'], (int, float)):\n",
        "          print(f\"- RMSE (original scale): ${best_metrics['RMSE_orig']:,.0f}\")\n",
        "     else:\n",
        "          print(f\"- RMSE (original scale): {best_metrics['RMSE_orig']}\")\n",
        "     if isinstance(best_metrics['MAE_orig'], (int, float)):\n",
        "          print(f\"- MAE (original scale): ${best_metrics['MAE_orig']:,.0f} (Average prediction error in dollars)\")\n",
        "     else:\n",
        "          print(f\"- MAE (original scale): {best_metrics['MAE_orig']}\")\n",
        "\n",
        "     print(\"\\nInterpretation:\")\n",
        "     print(f\"- The {best_model_name} model achieved the best predictive performance on the log-transformed target, explaining {best_metrics['R2']:.1%} of the variance.\")\n",
        "     if isinstance(best_metrics['MAE_orig'], (int, float)):\n",
        "          print(f\"- In practical terms, the model's predictions are, on average, about ${best_metrics['MAE_orig']:,.0f} off the actual sale price.\")\n",
        "     print(\"- Regularized models (Ridge) and tree-based ensembles (GradientBoosting, RandomForest) generally outperformed basic Linear Regression and Lasso (which performed poorly, likely due to default alpha).\")\n",
        "\n",
        "     print(\"\\nLimitations & Next Steps:\")\n",
        "     print(\"- Preprocessing: Ordinal features were treated as nominal (One-Hot Encoded); using Ordinal Encoding with correct mapping could improve performance. More feature engineering could be explored.\")\n",
        "     print(\"- Modeling: Only default or basic hyperparameters were used. Hyperparameter tuning (e.g., using GridSearchCV or RandomizedSearchCV) for the top models (Ridge, GradientBoosting, RandomForest) is likely to yield significant improvements.\")\n",
        "     print(\"- Feature Importance: Analysis of feature importances (especially for tree models) was not performed but would provide insights into price drivers.\")\n",
        "     print(\"- Advanced Models: Techniques like XGBoost, LightGBM, or stacking/ensembling could be explored for potentially higher accuracy.\")\n",
        "\n",
        "else:\n",
        "    print(\"Analysis did not complete successfully or evaluation results are missing. Conclusions cannot be finalized.\")\n",
        "\n",
        "\n",
        "# End of Regression Notebook\n",
        "print(\"\\n--- End of Regression Notebook ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwUiyrJVLPmj",
        "outputId": "a8d828d2-8c5f-4466-84bf-28085475b991"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Section 7: Conclusions ---\n",
            "Summary of the house price prediction project:\n",
            "- Data preprocessing involved handling numerous missing values (imputing based on meaning), feature engineering (TotalSF, Age, Remodeling flags), log-transforming the target variable ('SalePrice') and skewed features ('GrLivArea'), and One-Hot Encoding categorical features.\n",
            "- Five different regression models were trained: Linear Regression, Ridge, Lasso, Random Forest, and Gradient Boosting.\n",
            "- Models were evaluated using RMSE and MAE (on log and original scales) and R2 score.\n",
            "\n",
            "Best performing model (based on lowest RMSE_log): Ridge\n",
            "Key Performance Metrics for Best Model:\n",
            "- RMSE (log scale): 0.1366\n",
            "- R2 Score: 0.9000\n",
            "- RMSE (original scale): $25,844\n",
            "- MAE (original scale): $16,236 (Average prediction error in dollars)\n",
            "\n",
            "Interpretation:\n",
            "- The Ridge model achieved the best predictive performance on the log-transformed target, explaining 90.0% of the variance.\n",
            "- In practical terms, the model's predictions are, on average, about $16,236 off the actual sale price.\n",
            "- Regularized models (Ridge) and tree-based ensembles (GradientBoosting, RandomForest) generally outperformed basic Linear Regression and Lasso (which performed poorly, likely due to default alpha).\n",
            "\n",
            "Limitations & Next Steps:\n",
            "- Preprocessing: Ordinal features were treated as nominal (One-Hot Encoded); using Ordinal Encoding with correct mapping could improve performance. More feature engineering could be explored.\n",
            "- Modeling: Only default or basic hyperparameters were used. Hyperparameter tuning (e.g., using GridSearchCV or RandomizedSearchCV) for the top models (Ridge, GradientBoosting, RandomForest) is likely to yield significant improvements.\n",
            "- Feature Importance: Analysis of feature importances (especially for tree models) was not performed but would provide insights into price drivers.\n",
            "- Advanced Models: Techniques like XGBoost, LightGBM, or stacking/ensembling could be explored for potentially higher accuracy.\n",
            "\n",
            "--- End of Regression Notebook ---\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}