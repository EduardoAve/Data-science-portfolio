{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jWMPxDLSBD2X4nBc-4coOxqThWnYT5IH",
      "authorship_tag": "ABX9TyNfU96W3oAcH6cqsvjsf2w5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EduardoAve/Data-science-portfolio/blob/main/notebooks/variability_imputation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xY1_TyVeFTsx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- URLs de los archivos CSV en GitHub ---\n",
        "url_vulnerability = 'https://raw.githubusercontent.com/EduardoAve/Labour-well-being/refs/heads/main/data/01_raw-00/vulnerability.csv'\n",
        "url_dataset = 'https://raw.githubusercontent.com/EduardoAve/Labour-well-being/refs/heads/main/data/01_raw-00/Final_Dataset.csv'\n",
        "\n",
        "# --- Cargar los datos en los DataFrames ---\n",
        "\n",
        "# df1 cargará los datos de vulnerability.csv\n",
        "print(\"Cargando datos de vulnerability...\")\n",
        "df1 = pd.read_csv(url_vulnerability)\n",
        "\n",
        "# df2 cargará los datos de Final_Dataset.csv\n",
        "print(\"Cargando datos de Final_Dataset...\")\n",
        "df2 = pd.read_csv(url_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj-pQAbPFWx3",
        "outputId": "d7059164-ef59-4c57-badb-c5d68d1d4ccc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando datos de vulnerability...\n",
            "Cargando datos de Final_Dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# numpy es necesario para el redondeo\n",
        "import numpy as np\n",
        "# Importar KNNImputer para la imputación\n",
        "try:\n",
        "    from sklearn.impute import KNNImputer\n",
        "    sklearn_disponible = True\n",
        "except ImportError:\n",
        "    sklearn_disponible = False\n",
        "    print(\"Advertencia: La librería 'scikit-learn' no está instalada.\")\n",
        "    print(\"No se realizará la imputación KNN. Para habilitarla, ejecuta: pip install scikit-learn\")\n",
        "\n",
        "\n",
        "# --- Asumimos que df1 y df2 ya están cargados ---\n",
        "# Por ejemplo:\n",
        "# df1 = pd.read_csv('ruta/a/tu/df1.csv')\n",
        "# df2 = pd.read_csv('ruta/a/tu/df2.csv')\n",
        "# O desde Excel:\n",
        "# df1 = pd.read_excel('ruta/a/tu/archivo.xlsx', sheet_name='Hoja_df1')\n",
        "# df2 = pd.read_excel('ruta/a/tu/archivo.xlsx', sheet_name='Hoja_df2')\n",
        "\n",
        "# --- Verifica las formas iniciales (opcional pero recomendado) ---\n",
        "print(\"--- Formas Iniciales de los DataFrames Cargados ---\")\n",
        "try:\n",
        "    print(f\"Forma df1: {df1.shape}\")\n",
        "    print(f\"Forma df2: {df2.shape}\")\n",
        "    # Validar número de columnas esperado en df1\n",
        "    if df1.shape[1] != 45:\n",
        "        print(\"Advertencia: df1 no tiene las 45 columnas esperadas.\")\n",
        "    # Validar que df2 tenga al menos 45 columnas\n",
        "    if df2.shape[1] < 45:\n",
        "         print(\"Error: df2 tiene menos de 45 columnas, no se puede procesar.\")\n",
        "         # Podrías detener la ejecución aquí si es un error crítico\n",
        "         exit()\n",
        "    # Validar que tengan el mismo número de filas\n",
        "    if df1.shape[0] != df2.shape[0]:\n",
        "        print(\"Advertencia: df1 y df2 no tienen el mismo número de filas. El filtro podría no funcionar como se espera si los índices no coinciden perfectamente.\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"Error: Asegúrate de que las variables 'df1' y 'df2' existan y contengan tus DataFrames.\")\n",
        "    # Detener ejecución si los dataframes no existen\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrió un error al verificar los DataFrames: {e}\")\n",
        "    # Detener ejecución si hay problemas\n",
        "    exit()\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# --- Definir constantes ---\n",
        "num_preguntas_vulnerabilidad = 44\n",
        "# Criterio: Eliminar si hay MÁS de 4 nulos.\n",
        "# Por lo tanto, mantenemos si hay 4 o MENOS nulos.\n",
        "max_nulos_permitidos = 4\n",
        "\n",
        "# --- Identificar las columnas de preguntas en df2 ---\n",
        "# Son las 'num_preguntas_vulnerabilidad' columnas ANTES de la última columna de df2\n",
        "try:\n",
        "    total_cols_df2 = df2.shape[1]\n",
        "    inicio_preguntas_df2 = total_cols_df2 - 1 - num_preguntas_vulnerabilidad\n",
        "    fin_preguntas_df2 = total_cols_df2 - 1 # El índice final para iloc es exclusivo\n",
        "    columnas_preguntas_df2 = df2.iloc[:, inicio_preguntas_df2:fin_preguntas_df2]\n",
        "\n",
        "    # --- Calcular cuántas preguntas SÍ fueron contestadas (NaNs) por registro en df2 ---\n",
        "    # Contar valores NULOS en las columnas de preguntas de df2\n",
        "    nulos_por_registro = columnas_preguntas_df2.isnull().sum(axis=1)\n",
        "\n",
        "    # --- Crear el filtro: Mantener registros con 4 o MENOS valores nulos ---\n",
        "    # El criterio es eliminar si nulos > 4, por lo tanto, mantenemos si nulos <= 4\n",
        "    filtro_mantener = nulos_por_registro <= max_nulos_permitidos\n",
        "\n",
        "    print(f\"\\nSe aplicará filtro para MANTENER registros con {max_nulos_permitidos} o menos NULOS (preguntas no contestadas).\")\n",
        "    print(f\"Número de registros a mantener: {filtro_mantener.sum()}\")\n",
        "    print(f\"Número de registros a eliminar (con más de {max_nulos_permitidos} nulos): {len(df1) - filtro_mantener.sum()}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # --- Aplicar el filtro a ambos DataFrames ---\n",
        "    # Se asume que los índices de df1 y df2 están alineados\n",
        "    df1_depurado = df1[filtro_mantener].copy()\n",
        "    df2_depurado = df2[filtro_mantener].copy()\n",
        "\n",
        "    print(\"--- DataFrames Depurados (Antes de Imputación) ---\")\n",
        "    print(f\"Forma df1 depurado: {df1_depurado.shape}\")\n",
        "    print(f\"Forma df2 depurado: {df2_depurado.shape}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # --- Imputación KNN (si sklearn está disponible) ---\n",
        "    if sklearn_disponible and not df1_depurado.empty and not df2_depurado.empty:\n",
        "        print(\"--- Realizando Imputación KNN ---\")\n",
        "        # Configurar el imputador KNN (puedes ajustar n_neighbors)\n",
        "        imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "        # --- Imputar df1_depurado ---\n",
        "        print(\"Imputando df1...\")\n",
        "        # Seleccionar las columnas de preguntas (primeras num_preguntas_vulnerabilidad)\n",
        "        cols_preguntas_df1_idx = df1_depurado.columns[:num_preguntas_vulnerabilidad]\n",
        "        cols_a_imputar_df1 = df1_depurado[cols_preguntas_df1_idx]\n",
        "        col_vulnerabilidad_df1 = df1_depurado.iloc[:, -1] # Última columna\n",
        "\n",
        "        # Aplicar imputación\n",
        "        imputed_data_df1 = imputer.fit_transform(cols_a_imputar_df1)\n",
        "        # *** NUEVO: Redondear y convertir a entero ***\n",
        "        imputed_data_df1 = np.round(imputed_data_df1).astype(np.int64)\n",
        "        df_imputed_part_df1 = pd.DataFrame(imputed_data_df1, columns=cols_preguntas_df1_idx, index=df1_depurado.index)\n",
        "\n",
        "        # Reconstruir df1_depurado con datos imputados\n",
        "        df1_depurado = pd.concat([df_imputed_part_df1, col_vulnerabilidad_df1], axis=1)\n",
        "        print(\"df1 imputado y redondeado.\")\n",
        "\n",
        "        # --- Imputar df2_depurado ---\n",
        "        print(\"Imputando df2...\")\n",
        "        # Identificar columnas en df2_depurado (posiciones relativas pueden cambiar si df2 original tenía más de 45 cols)\n",
        "        total_cols_df2_depurado = df2_depurado.shape[1]\n",
        "        inicio_preguntas_df2_depurado = total_cols_df2_depurado - 1 - num_preguntas_vulnerabilidad\n",
        "        fin_preguntas_df2_depurado = total_cols_df2_depurado - 1\n",
        "\n",
        "        # Seleccionar partes de df2_depurado\n",
        "        cols_antes_df2 = df2_depurado.iloc[:, :inicio_preguntas_df2_depurado] # Columnas antes de las preguntas\n",
        "        cols_preguntas_df2_idx = df2_depurado.columns[inicio_preguntas_df2_depurado:fin_preguntas_df2_depurado]\n",
        "        cols_a_imputar_df2 = df2_depurado[cols_preguntas_df2_idx]\n",
        "        col_vulnerabilidad_df2 = df2_depurado.iloc[:, -1] # Última columna\n",
        "\n",
        "        # Aplicar imputación\n",
        "        # Nota: Usamos el mismo imputer ajustado en df1 o ajustamos uno nuevo.\n",
        "        imputed_data_df2 = imputer.fit_transform(cols_a_imputar_df2)\n",
        "        # *** NUEVO: Redondear y convertir a entero ***\n",
        "        imputed_data_df2 = np.round(imputed_data_df2).astype(np.int64)\n",
        "        df_imputed_part_df2 = pd.DataFrame(imputed_data_df2, columns=cols_preguntas_df2_idx, index=df2_depurado.index)\n",
        "\n",
        "        # Reconstruir df2_depurado con datos imputados\n",
        "        partes_df2 = []\n",
        "        if not cols_antes_df2.empty:\n",
        "            partes_df2.append(cols_antes_df2)\n",
        "        partes_df2.append(df_imputed_part_df2)\n",
        "        partes_df2.append(col_vulnerabilidad_df2)\n",
        "        df2_depurado = pd.concat(partes_df2, axis=1)\n",
        "        print(\"df2 imputado y redondeado.\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        print(\"--- DataFrames Depurados e Imputados (Enteros) ---\")\n",
        "        print(f\"Forma df1 depurado e imputado: {df1_depurado.shape}\")\n",
        "        print(f\"Forma df2 depurado e imputado: {df2_depurado.shape}\")\n",
        "        # Verificar si quedan NaNs en las columnas imputadas (no debería haber)\n",
        "        print(f\"NaNs restantes en preguntas df1: {df1_depurado.iloc[:,:num_preguntas_vulnerabilidad].isnull().sum().sum()}\")\n",
        "        print(f\"NaNs restantes en preguntas df2: {df2_depurado.iloc[:, inicio_preguntas_df2_depurado:fin_preguntas_df2_depurado].isnull().sum().sum()}\")\n",
        "        # Verificar tipo de dato (debería ser entero)\n",
        "        print(f\"Tipo de dato ejemplo en preguntas df1: {df1_depurado.iloc[0, 0].dtype}\")\n",
        "        print(f\"Tipo de dato ejemplo en preguntas df2: {df2_depurado.iloc[0, inicio_preguntas_df2_depurado].dtype}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    elif not sklearn_disponible:\n",
        "         print(\"Imputación KNN omitida porque 'scikit-learn' no está instalado.\")\n",
        "    elif df1_depurado.empty or df2_depurado.empty:\n",
        "         print(\"Imputación KNN omitida porque uno o ambos DataFrames depurados están vacíos.\")\n",
        "\n",
        "\n",
        "    # --- Guardar los DataFrames (depurados y posiblemente imputados) en un mismo archivo Excel ---\n",
        "    nombre_archivo_excel = 'dataset_vulnerability_new.xlsx' # Nombre de archivo\n",
        "\n",
        "    try:\n",
        "        with pd.ExcelWriter(nombre_archivo_excel, engine='openpyxl') as writer:\n",
        "            # Hoja 1: df2 con nombre 'dataset'\n",
        "            df2_depurado.to_excel(writer, sheet_name='dataset', index=False)\n",
        "            # Hoja 2: df1 con nombre 'vulnerability'\n",
        "            df1_depurado.to_excel(writer, sheet_name='vulnerability', index=False)\n",
        "\n",
        "        print(f\"Archivos depurados e imputados (enteros) guardados correctamente en '{nombre_archivo_excel}'\")\n",
        "        print(f\"  - Hoja 'dataset': Contiene df2 depurado e imputado.\")\n",
        "        print(f\"  - Hoja 'vulnerability': Contiene df1 depurado e imputado.\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"\\nError: Necesitas instalar la librería 'openpyxl' para escribir archivos .xlsx.\")\n",
        "        print(\"Ejecuta: pip install openpyxl\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nOcurrió un error al guardar el archivo Excel: {e}\")\n",
        "\n",
        "except IndexError:\n",
        "     print(f\"Error: Problema al acceder a las columnas de df2. Verifica que df2 tenga al menos {num_preguntas_vulnerabilidad + 1} columnas.\")\n",
        "except Exception as e:\n",
        "     print(f\"Ocurrió un error inesperado durante el procesamiento: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl7B77EYW96A",
        "outputId": "bcef07be-c15b-4265-a84a-a576bf051406"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Formas Iniciales de los DataFrames Cargados ---\n",
            "Forma df1: (2747, 45)\n",
            "Forma df2: (2747, 132)\n",
            "------------------------------\n",
            "\n",
            "Se aplicará filtro para MANTENER registros con 4 o menos NULOS (preguntas no contestadas).\n",
            "Número de registros a mantener: 2708\n",
            "Número de registros a eliminar (con más de 4 nulos): 39\n",
            "------------------------------\n",
            "--- DataFrames Depurados (Antes de Imputación) ---\n",
            "Forma df1 depurado: (2708, 45)\n",
            "Forma df2 depurado: (2708, 132)\n",
            "------------------------------\n",
            "--- Realizando Imputación KNN ---\n",
            "Imputando df1...\n",
            "df1 imputado y redondeado.\n",
            "Imputando df2...\n",
            "df2 imputado y redondeado.\n",
            "------------------------------\n",
            "--- DataFrames Depurados e Imputados (Enteros) ---\n",
            "Forma df1 depurado e imputado: (2708, 45)\n",
            "Forma df2 depurado e imputado: (2708, 132)\n",
            "NaNs restantes en preguntas df1: 0\n",
            "NaNs restantes en preguntas df2: 0\n",
            "Tipo de dato ejemplo en preguntas df1: int64\n",
            "Tipo de dato ejemplo en preguntas df2: int64\n",
            "------------------------------\n",
            "Archivos depurados e imputados (enteros) guardados correctamente en 'dataset_vulnerability_new.xlsx'\n",
            "  - Hoja 'dataset': Contiene df2 depurado e imputado.\n",
            "  - Hoja 'vulnerability': Contiene df1 depurado e imputado.\n"
          ]
        }
      ]
    }
  ]
}